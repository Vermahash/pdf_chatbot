{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef37a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, uuid, pathlib, subprocess, json\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5c4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TOP_K = 5\n",
    "MIN_SCORE = 0.5\n",
    "OLLAMA_MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b97e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = SentenceTransformer(EMBED_MODEL)\n",
    "chroma = Client(Settings(anonymized_telemetry=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4a9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_reader.py\n",
    "import io\n",
    "import fitz  # PyMuPDF\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_pages(pdf_bytes: bytes) -> list[str]:\n",
    "    # Try PyMuPDF first (usually best); fall back to pypdf\n",
    "    try:\n",
    "        doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "        pages = [doc[i].get_text() or \"\" for i in range(len(doc))]\n",
    "        doc.close()\n",
    "        return pages\n",
    "    except Exception:\n",
    "        reader = PdfReader(io.BytesIO(pdf_bytes))\n",
    "        return [(p.extract_text() or \"\") for p in reader.pages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b5f6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "_client = Client(Settings(anonymized_telemetry=False))\n",
    "_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# In-memory cache so we can run exact-match before vector search\n",
    "CHUNK_CACHE: dict[str, List[Dict[str, Any]]] = {}\n",
    "\n",
    "BULLET_RE = re.compile(r'^\\s*([\\-•\\*\\u2022]|\\d+[\\.)])\\s+')\n",
    "\n",
    "def _split_paragraphs(text: str) -> List[str]:\n",
    "    t = re.sub(r'\\r\\n?', '\\n', text or '')\n",
    "    # split on blank lines\n",
    "    paras = [p.strip() for p in re.split(r'\\n\\s*\\n', t) if p.strip()]\n",
    "    return paras\n",
    "\n",
    "def _chunk_paragraphs_with_bullets(page_text: str) -> List[str]:\n",
    "    paras = _split_paragraphs(page_text)\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(paras):\n",
    "        cur = paras[i]\n",
    "        # \"Heading\" heuristic: shortish line, no trailing punctuation (tweak if needed)\n",
    "        is_heading = (len(cur) < 120) and (not cur.endswith(('.', '!', '?')))\n",
    "        if is_heading and i + 1 < len(paras):\n",
    "            j = i + 1\n",
    "            bullets = []\n",
    "            while j < len(paras) and BULLET_RE.match(paras[j]):\n",
    "                bullets.append(paras[j])\n",
    "                j += 1\n",
    "            if bullets:\n",
    "                # group heading + its bullet block\n",
    "                out.append(cur + \"\\n\" + \"\\n\".join(bullets))\n",
    "                i = j\n",
    "                continue\n",
    "        out.append(cur)\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "def build_index(doc_id: str, filename: str, pages: List[str]):\n",
    "    col = _client.get_or_create_collection(name=f\"doc_{doc_id}\")\n",
    "    ids, docs, metas = [], [], []\n",
    "    cache_items: List[Dict[str, Any]] = []\n",
    "\n",
    "    for page_no, page_text in enumerate(pages, start=1):\n",
    "        for n, chunk in enumerate(_chunk_paragraphs_with_bullets(page_text)):\n",
    "            cid = f\"{doc_id}_{page_no}_{n}\"\n",
    "            ids.append(cid)\n",
    "            docs.append(chunk)\n",
    "            meta = {\"page\": page_no, \"filename\": filename, \"cid\": cid}\n",
    "            metas.append(meta)\n",
    "            cache_items.append({\"text\": chunk, \"meta\": meta})\n",
    "\n",
    "    if docs:\n",
    "        vecs = _model.encode(docs, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        col.add(ids=ids, documents=docs, metadatas=metas, embeddings=vecs.tolist())\n",
    "\n",
    "    # keep paragraph/bullet chunks for exact-match pass\n",
    "    CHUNK_CACHE[doc_id] = cache_items\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadd5c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# indexer.py\\nfrom chromadb import Client\\nfrom chromadb.config import Settings\\nfrom sentence_transformers import SentenceTransformer\\n\\n_client = Client(Settings(anonymized_telemetry=False))\\n_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # fast + good baseline\\n\\ndef build_index(doc_id: str, filename: str, pages: list[str]):\\n    col = _client.get_or_create_collection(name=f\"doc_{doc_id}\")\\n\\n    ids, docs, metas, embeds = [], [], [], []\\n    for page_num, page_text in enumerate(pages, start=1):\\n        for j, chunk in enumerate(chunk_text(page_text)):\\n            ids.append(f\"{doc_id}_{page_num}_{j}\")\\n            docs.append(chunk)\\n            metas.append({\"page\": page_num, \"filename\": filename})\\n    if docs:\\n        embeds = _model.encode(docs, convert_to_numpy=True, normalize_embeddings=True)\\n        col.add(ids=ids, documents=docs, metadatas=metas, embeddings=embeds.tolist())\\n    return col\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# indexer.py\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "_client = Client(Settings(anonymized_telemetry=False))\n",
    "_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # fast + good baseline\n",
    "\n",
    "def build_index(doc_id: str, filename: str, pages: list[str]):\n",
    "    col = _client.get_or_create_collection(name=f\"doc_{doc_id}\")\n",
    "\n",
    "    ids, docs, metas, embeds = [], [], [], []\n",
    "    for page_num, page_text in enumerate(pages, start=1):\n",
    "        for j, chunk in enumerate(chunk_text(page_text)):\n",
    "            ids.append(f\"{doc_id}_{page_num}_{j}\")\n",
    "            docs.append(chunk)\n",
    "            metas.append({\"page\": page_num, \"filename\": filename})\n",
    "    if docs:\n",
    "        embeds = _model.encode(docs, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        col.add(ids=ids, documents=docs, metadatas=metas, embeddings=embeds.tolist())\n",
    "    return col\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6814538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.py\n",
    "from typing import List, Dict, Any\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "\n",
    "_client = Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "def retrieve(doc_id: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    col = _client.get_or_create_collection(name=f\"doc_{doc_id}\")\n",
    "    if col.count() == 0:\n",
    "        return []\n",
    "    res = col.query(query_texts=[query], n_results=top_k,\n",
    "                    include=[\"documents\",\"metadatas\",\"distances\",\"embeddings\"])\n",
    "    items = []\n",
    "    if res and res[\"documents\"]:\n",
    "        docs = res[\"documents\"][0]\n",
    "        metas = res[\"metadatas\"][0]\n",
    "        dists = res[\"distances\"][0]\n",
    "        for doc, meta, dist in zip(docs, metas, dists):\n",
    "            # Chroma's distance depends on embedding_fn; with our manual embeddings,\n",
    "            # it's cosine distance in [0,2]. Convert to similarity in [0,1] roughly:\n",
    "            # sim = 1 - (dist / 2)\n",
    "            sim = 1 - (float(dist) / 2.0)\n",
    "            items.append({\"text\": doc, \"meta\": meta, \"score\": float(np.clip(sim, 0.0, 1.0))})\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a4cc31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# qa.py\n",
    "from typing import Dict, Any\n",
    "\n",
    "MIN_SCORE = 0.40\n",
    "TOP_K = 5\n",
    "\n",
    "def answer(question: str, doc_id: str) -> Dict[str, Any]:\n",
    "    hits = retrieve(doc_id, question, TOP_K)\n",
    "    if not hits or hits[0][\"score\"] < MIN_SCORE:\n",
    "        return {\n",
    "            \"answer\": \"I do not have an answer to that question because the PDF doesn’t contain it (or I can’t retrieve it reliably).\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "    # Extractive baseline: return stitched text from top chunks\n",
    "    context = \"\\n\\n---\\n\\n\".join([h[\"text\"] for h in hits[:2]])  # keep it short for readability\n",
    "    pages = sorted({h[\"meta\"][\"page\"] for h in hits[:2]})\n",
    "    return {\n",
    "        \"answer\": context,  # replace later with LLM-generated summary over context, if you want\n",
    "        \"sources\": [{\"page\": p} for p in pages],\n",
    "        \"scores\": [round(h[\"score\"],3) for h in hits[:2]]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "MIN_SCORE = 0.45     # raise to reduce bloat; lower to refuse less\n",
    "TOP_K = 5\n",
    "OLLAMA_MODEL = \"mistral\"  # or \"llama3\"\n",
    "\n",
    "def _ollama(prompt: str) -> str:\n",
    "    try:\n",
    "        p = run([\"ollama\", \"run\", OLLAMA_MODEL, prompt], stdout=PIPE, stderr=PIPE, text=True)\n",
    "        return (p.stdout or \"\").strip()\n",
    "    except FileNotFoundError:\n",
    "        return \"\"  # if ollama isn’t available, we’ll return extractive text\n",
    "\n",
    "def _exact_match_pass(doc_id: str, query: str) -> Dict[str, Any] | None:\n",
    "    \"\"\"\n",
    "    If the query matches a heading inside a heading+bullets chunk,\n",
    "    return ONLY the following 1–3 lines (the bullets) as the answer.\n",
    "    \"\"\"\n",
    "    chunks = CHUNK_CACHE.get(doc_id, [])\n",
    "    q = (query or \"\").strip().lower()\n",
    "    if not q:\n",
    "        return None\n",
    "\n",
    "    for ch in chunks:\n",
    "        text = ch[\"text\"]\n",
    "        if q in text.lower():\n",
    "            # split into non-empty lines\n",
    "            lines = [l for l in text.splitlines() if l.strip()]\n",
    "            # find the matched line (likely the heading)\n",
    "            try:\n",
    "                idx = next(i for i, l in enumerate(lines) if q in l.lower())\n",
    "            except StopIteration:\n",
    "                continue\n",
    "            # take next 1–3 lines as the answer\n",
    "            following: List[str] = []\n",
    "            for l in lines[idx+1:]:\n",
    "                following.append(l)\n",
    "                if len(following) >= 3:  # tweak if you want more\n",
    "                    break\n",
    "            if following:\n",
    "                snippet = \"\\n\".join(following)\n",
    "            else:\n",
    "                # fallback: return the chunk if no bullet follows\n",
    "                snippet = text\n",
    "            return {\n",
    "                \"answer\": snippet,\n",
    "                \"sources\": [{\"page\": ch[\"meta\"][\"page\"]}],\n",
    "                \"scores\": [1.0],\n",
    "            }\n",
    "    return None\n",
    "\n",
    "def answer(question: str, doc_id: str, history: List[Dict[str,str]] | None = None) -> Dict[str, Any]:\n",
    "    # 1) exact-match first – handles “heading → bullet” cases precisely\n",
    "    em = _exact_match_pass(doc_id, question)\n",
    "    if em:\n",
    "        return em\n",
    "\n",
    "    # 2) semantic retrieve (fallback)\n",
    "    hits = retrieve(doc_id, question, TOP_K)\n",
    "    if not hits or hits[0][\"score\"] < MIN_SCORE:\n",
    "        return {\n",
    "            \"answer\": \"I cannot find that in the PDF.\",\n",
    "            \"sources\": [],\n",
    "            \"scores\": []\n",
    "        }\n",
    "\n",
    "    # 3) keep a small, tight context (best 1–2 hits)\n",
    "    top = hits[:2]\n",
    "    context = \"\\n\\n---\\n\\n\".join([h[\"text\"] for h in top])\n",
    "    pages = sorted({h[\"meta\"][\"page\"] for h in top})\n",
    "\n",
    "    # 4) ask Ollama with strict grounding; if Ollama not available, return extractive text\n",
    "    prompt = f\"\"\"\n",
    "You are a PDF-grounded assistant.\n",
    "Use ONLY the context from the PDF to answer.\n",
    "If the answer is not in the context, say exactly:\n",
    "\"I cannot find that in the PDF.\"\n",
    "Cite page numbers {pages}.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\".strip()\n",
    "\n",
    "    llm_answer = _ollama(prompt)\n",
    "    if not llm_answer:\n",
    "        # local fallback: just return extractive context\n",
    "        return {\n",
    "            \"answer\": context,\n",
    "            \"sources\": [{\"page\": p} for p in pages],\n",
    "            \"scores\": [round(h[\"score\"], 3) for h in top],\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"answer\": llm_answer,\n",
    "        \"sources\": [{\"page\": p} for p in pages],\n",
    "        \"scores\": [round(h[\"score\"], 3) for h in top],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b72bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Answer with PDF-only grounding\n",
    "def answer(question: str, doc_id: str):\n",
    "    hits = retrieve(doc_id, question, TOP_K)\n",
    "    if not hits or hits[0][\"score\"] < MIN_SCORE:\n",
    "        return {\n",
    "            \"answer\": \"I cannot find that in the PDF.\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "    context = \"\\n\\n---\\n\\n\".join([h[\"text\"] for h in hits])\n",
    "    pages = sorted({h[\"meta\"][\"page\"] for h in hits})\n",
    "    prompt = f\"\"\"\n",
    "You are a PDF-grounded assistant.\n",
    "Use ONLY the context from the PDF below to answer the question.\n",
    "If the answer is not in the context, say exactly:\n",
    "\"I cannot find that in the PDF.\"\n",
    "Always cite page numbers {pages} in your answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    llm_answer = call_ollama(prompt)\n",
    "    return {\n",
    "        \"answer\": llm_answer,\n",
    "        \"sources\": [f\"p.{p}\" for p in pages]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80903164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_paragraphs(text: str) -> list[str]:\n",
    "    # normalize newlines\n",
    "    t = re.sub(r'\\r\\n?', '\\n', text or '')\n",
    "    # split on blank lines\n",
    "    paras = [p.strip() for p in re.split(r'\\n\\s*\\n', t) if p.strip()]\n",
    "    return paras\n",
    "\n",
    "BULLET_RE = re.compile(r'^\\s*([\\-•\\*\\u2022]|\\d+[\\.)])\\s+')\n",
    "\n",
    "def chunk_paragraphs_with_bullets(page_text: str) -> list[str]:\n",
    "    paras = split_paragraphs(page_text)\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(paras):\n",
    "        cur = paras[i]\n",
    "        # if this para looks like a heading (short, Title Case, ends without punctuation), group with the bullet block after it\n",
    "        is_heading = (len(cur) < 120 and cur.endswith(('.', ':')) is False)\n",
    "        if is_heading and i + 1 < len(paras):\n",
    "            # collect consecutive bullet paragraphs\n",
    "            j = i + 1\n",
    "            bullets = []\n",
    "            while j < len(paras) and BULLET_RE.match(paras[j]):\n",
    "                bullets.append(paras[j])\n",
    "                j += 1\n",
    "            if bullets:\n",
    "                out.append(cur + \"\\n\" + \"\\n\".join(bullets))\n",
    "                i = j\n",
    "                continue\n",
    "        out.append(cur)\n",
    "        i += 1\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a778e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_pass(chunks_with_meta, query: str):\n",
    "    q = query.strip().lower()\n",
    "    for ch in chunks_with_meta:\n",
    "        text = ch[\"text\"]\n",
    "        if q and q in text.lower():\n",
    "            # If it’s a heading+bullets chunk, extract only the first bullet/line after heading\n",
    "            lines = [l for l in text.splitlines() if l.strip()]\n",
    "            # find heading line index\n",
    "            idx = next((k for k,l in enumerate(lines) if q in l.lower()), None)\n",
    "            if idx is not None:\n",
    "                # return the next non-heading line(s), capped\n",
    "                following = []\n",
    "                for l in lines[idx+1:]:\n",
    "                    following.append(l)\n",
    "                    # stop after 1–3 bullet/lines; tweak to taste\n",
    "                    if len(following) >= 3: break\n",
    "                snippet = \"\\n\".join(following) if following else text\n",
    "                return {\"text\": snippet, \"meta\": ch[\"meta\"], \"score\": 1.0}\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d11c1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_pdf_only(question, doc_id):\n",
    "    # 0) load paragraph-level chunks for doc_id (you already have them in your collection or in-memory)\n",
    "    chunks = load_doc_chunks(doc_id)  # each: {\"text\":..., \"meta\":{\"page\":...}}\n",
    "    \n",
    "    # 1) exact match first\n",
    "    em = exact_match_pass(chunks, question)\n",
    "    if em:\n",
    "        return {\n",
    "            \"answer\": em[\"text\"],\n",
    "            \"sources\": [f\"p.{em['meta']['page']}\"]\n",
    "        }\n",
    "    \n",
    "    # 2) semantic retrieve + gating (your existing retrieve)\n",
    "    hits = retrieve(doc_id, question, top_k=5)\n",
    "    if not hits or hits[0][\"score\"] < MIN_SCORE:\n",
    "        return {\"answer\": \"I cannot find that in the PDF.\", \"sources\": []}\n",
    "    \n",
    "    # 3) small context → Ollama with strict prompt (if you still want generated prose)\n",
    "    context = \"\\n\\n---\\n\\n\".join([h[\"text\"] for h in hits[:2]])\n",
    "    pages = sorted({h[\"meta\"][\"page\"] for h in hits[:2]})\n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the context from the PDF to answer.\n",
    "If missing, say: \"I cannot find that in the PDF.\"\n",
    "Cite pages {pages}.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    llm_answer = call_ollama(prompt)\n",
    "    return {\"answer\": llm_answer, \"sources\": [f\"p.{p}\" for p in pages]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e79b46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import pathlib\n",
    "import sys\n",
    "import subprocess\n",
    "import textwrap\n",
    "\"\"\"\n",
    "def load_pdf(path: str) -> bytes:\n",
    "    return pathlib.Path(path).read_bytes()\n",
    "\n",
    "def main():\n",
    "    pdf_path = input(\"Path to PDF: \").strip()\n",
    "    q = None\n",
    "    raw = load_pdf(pdf_path)\n",
    "    pages = extract_pages(raw)\n",
    "    print(f\"Pages extracted: {len(pages)}\")\n",
    "\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    build_index(doc_id, filename=pdf_path.split(\"/\")[-1], pages=pages)\n",
    "    print(\"Indexed. Ask questions (blank to exit).\")\n",
    "\n",
    "    while True:\n",
    "        q = input(\"\\nQ: \").strip()\n",
    "        if not q:\n",
    "            break\n",
    "        resp = answer(q, doc_id)\n",
    "        print(\"\\nA:\", resp[\"answer\"])\n",
    "        if resp[\"sources\"]:\n",
    "            print(\"Sources:\", \", \".join([f\"p.{s['page']}\" for s in resp[\"sources\"]]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "USE_CQR = True            # Turn on conversational query reformulation\n",
    "OLLAMA_MODEL = \"mistral\"  # or \"llama3\"\n",
    "\n",
    "def load_pdf(path: str) -> bytes:\n",
    "    return pathlib.Path(path).read_bytes()\n",
    "\n",
    "# --------- Ollama helpers (local LLM) ----------\n",
    "def _ollama_run(model: str, prompt: str) -> str:\n",
    "    \"\"\"Run a one-shot prompt against a local Ollama model.\"\"\"\n",
    "    try:\n",
    "        res = subprocess.run(\n",
    "            [\"ollama\", \"run\", model, prompt],\n",
    "            capture_output=True, text=True, check=False\n",
    "        )\n",
    "        return res.stdout.strip()\n",
    "    except FileNotFoundError:\n",
    "        # Ollama not found or not on PATH\n",
    "        return \"\"\n",
    "\n",
    "def reformulate_with_ollama(history, new_q, model=OLLAMA_MODEL, turns=3) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite the user's new question into a standalone, contextful query\n",
    "    using the last N user turns from history.\n",
    "    \"\"\"\n",
    "    if not USE_CQR or not history:\n",
    "        return new_q\n",
    "\n",
    "    # Pull last N user turns\n",
    "    prior_user_turns = [h[\"content\"] for h in history if h[\"role\"] == \"user\"][-turns:]\n",
    "    if not prior_user_turns:\n",
    "        return new_q\n",
    "\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    Rewrite the user's question into a single, self-contained query.\n",
    "    Use the brief conversation context to replace pronouns and vague references.\n",
    "    Do NOT answer; only rewrite the question. Keep it concise and specific.\n",
    "\n",
    "    Context:\n",
    "    {chr(10).join(f\"- {p}\" for p in prior_user_turns)}\n",
    "\n",
    "    New question:\n",
    "    {new_q}\n",
    "\n",
    "    Rewritten standalone question:\n",
    "    \"\"\").strip()\n",
    "\n",
    "    rewritten = _ollama_run(model, prompt).strip()\n",
    "    # Fallback to original if Ollama isn't available / returns empty\n",
    "    return rewritten if rewritten else new_q\n",
    "\n",
    "# --------- Pretty helpers ----------\n",
    "def _pretty_sources(sources):\n",
    "    \"\"\"\n",
    "    Accepts either a list of dicts like {'page': 2} or a list of 'p.2' strings.\n",
    "    Returns a pretty string like 'p.2, p.3'.\n",
    "    \"\"\"\n",
    "    if not sources:\n",
    "        return \"\"\n",
    "    out = []\n",
    "    for s in sources:\n",
    "        if isinstance(s, dict) and \"page\" in s:\n",
    "            out.append(f\"p.{s['page']}\")\n",
    "        elif isinstance(s, str):\n",
    "            out.append(s if s.startswith(\"p.\") else f\"p.{s}\")\n",
    "    return \", \".join(out)\n",
    "\n",
    "def _print_help():\n",
    "    print(textwrap.dedent(\"\"\"\n",
    "    Commands:\n",
    "      :help         Show this help\n",
    "      :new          Load a new PDF (re-index)\n",
    "      :quit         Exit\n",
    "      (blank line)  Also exits\n",
    "\n",
    "    Just type your question to query the current PDF.\n",
    "    \"\"\").strip())\n",
    "\n",
    "def main():\n",
    "    # ---- Load & index a PDF ----\n",
    "    pdf_path = input(\"Path to PDF: \").strip()\n",
    "    if not pdf_path:\n",
    "        print(\"No file provided. Exiting.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    raw = load_pdf(pdf_path)\n",
    "    pages = extract_pages(raw)\n",
    "    print(f\"Pages extracted: {len(pages)}\")\n",
    "\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    build_index(doc_id, filename=pathlib.Path(pdf_path).name, pages=pages)\n",
    "    print(\"Indexed. Ask questions (':help' for commands, blank to exit).\")\n",
    "\n",
    "    # ---- Conversational history ----\n",
    "    # We keep a simple list of {'role': 'user'/'assistant', 'content': str}\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        q = input(\"\\nQ: \").strip()\n",
    "\n",
    "        # Commands\n",
    "        if q in (\"\", \":quit\", \":q\", \":exit\"):\n",
    "            print(\"Bye.\")\n",
    "            break\n",
    "        if q == \":help\":\n",
    "            _print_help()\n",
    "            continue\n",
    "        if q == \":new\":\n",
    "            # Load a new PDF and re-index\n",
    "            pdf_path = input(\"Path to NEW PDF: \").strip()\n",
    "            if not pdf_path:\n",
    "                print(\"No file provided. Keeping current document.\")\n",
    "                continue\n",
    "            try:\n",
    "                raw = load_pdf(pdf_path)\n",
    "                pages = extract_pages(raw)\n",
    "                print(f\"Pages extracted: {len(pages)}\")\n",
    "                doc_id = str(uuid.uuid4())\n",
    "                build_index(doc_id, filename=pathlib.Path(pdf_path).name, pages=pages)\n",
    "                history.clear()\n",
    "                print(\"Re-indexed. You can ask questions now.\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load/index new PDF: {e}\")\n",
    "                continue\n",
    "\n",
    "        # ---- Conversational query reformulation (CQR) ----\n",
    "        standalone_q = reformulate_with_ollama(history, q) if USE_CQR else q\n",
    "\n",
    "        # ---- Ask your existing QA function ----\n",
    "        # Try passing history if your answer() supports it;\n",
    "        # otherwise fall back to the original signature.\n",
    "        try:\n",
    "            resp = answer(standalone_q, doc_id, history=history)  # type: ignore\n",
    "        except TypeError:\n",
    "            resp = answer(standalone_q, doc_id)\n",
    "\n",
    "        # ---- Print answer + sources ----\n",
    "        ans = resp.get(\"answer\", \"\")\n",
    "        print(\"\\nA:\", ans)\n",
    "\n",
    "        sources = resp.get(\"sources\", [])\n",
    "        pretty_src = _pretty_sources(sources)\n",
    "        if pretty_src:\n",
    "            print(\"Sources:\", pretty_src)\n",
    "\n",
    "        # (Optional) show scores if present\n",
    "        scores = resp.get(\"scores\")\n",
    "        if scores:\n",
    "            print(\"Scores:\", scores)\n",
    "\n",
    "        # ---- Update history ----\n",
    "        history.append({\"role\": \"user\", \"content\": q})\n",
    "        history.append({\"role\": \"assistant\", \"content\": ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf516f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages extracted: 3\n",
      "Indexed. Ask questions (':help' for commands, blank to exit).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'call_ollama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     resp = \u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandalone_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: answer() got an unexpected keyword argument 'history'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    166\u001b[39m     resp = answer(standalone_q, doc_id, history=history)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     resp = \u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandalone_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# ---- Print answer + sources ----\u001b[39;00m\n\u001b[32m    171\u001b[39m ans = resp.get(\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36manswer\u001b[39m\u001b[34m(question, doc_id)\u001b[39m\n\u001b[32m     10\u001b[39m     pages = \u001b[38;5;28msorted\u001b[39m({h[\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hits})\n\u001b[32m     11\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33mYou are a PDF-grounded assistant.\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33mUse ONLY the context from the PDF below to answer the question.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     llm_answer = \u001b[43mcall_ollama\u001b[49m(prompt)\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: llm_answer,\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msources\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mp.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pages]\n\u001b[32m     28\u001b[39m     }\n",
      "\u001b[31mNameError\u001b[39m: name 'call_ollama' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6198670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
